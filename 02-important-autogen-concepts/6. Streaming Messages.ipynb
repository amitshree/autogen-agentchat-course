{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Agent Messages with Token Streaming\n",
    "\n",
    "You can stream messages generated by an agent in real-time using either `on_messages_stream()` or `run_stream()`. Additionally, you can stream the individual tokens generated by the underlying model.\n",
    "\n",
    "### Token Streaming with `model_client_stream=True`\n",
    "\n",
    "* Setting `model_client_stream=True` enables token streaming from the model client.\n",
    "* This will cause the agent to yield `ModelClientStreamingChunkEvent` messages within the `on_messages_stream()` and `run_stream()` generators.\n",
    "* These `ModelClientStreamingChunkEvent` messages contain the individual tokens as they are generated by the model.\n",
    "* This provides a very granular view of the model's output.\n",
    "\n",
    "**Important Note:**\n",
    "\n",
    "* The underlying model API must support streaming tokens for this feature to function.\n",
    "* Consult your model provider's documentation to confirm token streaming capabilities.\n",
    "\n",
    "### `on_messages_stream()`\n",
    "\n",
    "* This method provides an asynchronous generator.\n",
    "* It yields each individual message produced by the agent.\n",
    "* If `model_client_stream=True`, it will also yield `ModelClientStreamingChunkEvent` messages.\n",
    "* The final item yielded is the complete response message, accessible through the `chat_message` attribute.\n",
    "* This allows you to observe the agent's thought process, actions, and token generation as they occur.\n",
    "* For example, you can use `Console` to print these messages to the console as they are generated.\n",
    "* You can observe the agent using tools like `web_search` and see the results that influence its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from autogen_agentchat.ui import Console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o-mini\",\n",
    ")\n",
    "agent = AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    model_client=model_client,\n",
    "    model_client_stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def assistant_run_stream() -> None:\n",
    "    # Option 1: read each message from the stream (as shown in the previous example).\n",
    "    # async for message in agent.on_messages_stream(\n",
    "    #     [TextMessage(content=\"Share information about azure ai search.\", source=\"user\")],\n",
    "    #     cancellation_token=CancellationToken(),\n",
    "    # ):\n",
    "    #     print(message)\n",
    "\n",
    "    # Option 2: use Console to print all messages as they appear.\n",
    "    await Console(\n",
    "        agent.on_messages_stream(\n",
    "            [TextMessage(content=\"Share information about azure ai search.\", source=\"user\")],\n",
    "            cancellation_token=CancellationToken(),\n",
    "        ),\n",
    "        output_stats=True,  # Enable stats printing.\n",
    "    )\n",
    "\n",
    "\n",
    "# Use asyncio.run(assistant_run_stream()) when running in a script.\n",
    "await assistant_run_stream()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
