{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Agents - Model Clients\n",
    "\n",
    "## Introduction\n",
    "In many AI applications, agents require access to LLM model services such as OpenAI, Azure OpenAI, or local models like Ollama. Since different providers have varying APIs, `autogen-core` implements a protocol for model clients, while `autogen-ext` provides implementations for popular services. `AgentChat` can use these model clients to interact with the model services seamlessly.\n",
    "\n",
    "This document provides an overview of how to integrate and use OpenAI and Azure OpenAI client within AutoGen agent framework.\n",
    "\n",
    "---\n",
    "\n",
    "## OpenAI Model Client\n",
    "\n",
    "To access OpenAI models, we have installed the `openai` extension by mentioning `autogen-ext[openai]` in requirements.txt file.\n",
    "\n",
    "### Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_core.models import UserMessage\n",
    "\n",
    "openai_model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    # api_key=\"sk-...\",  # Optional if OPENAI_API_KEY environment variable is set.\n",
    ")\n",
    "\n",
    "result = await openai_model_client.create([UserMessage(content=\"Tell me a joke.\", source=\"user\")])\n",
    "print(\"OpenAI Response:\", result.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure OpenAI Model Client\n",
    "\n",
    "To use Azure OpenAI, install the required extensions:\n",
    "\n",
    "```bash\n",
    "pip install \"autogen-ext[openai,azure]\"\n",
    "```\n",
    "\n",
    "### Implementation:\n",
    "Update environment variables below based on settings in azure portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from autogen_core.models import UserMessage\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() # will search for .env file in local folder and load variables \n",
    "\n",
    "azure_openai_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "\n",
    "az_model_client = AzureOpenAIChatCompletionClient(\n",
    "    azure_deployment=\"{your-azure-deployment}\", # deployment name from Azure OpenAI Deployments tab\n",
    "    model=\"gpt-4o\", # name of the model\n",
    "    api_version=\"2024-06-01\", \n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_key=azure_openai_key,  \n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "result = await az_model_client.create([UserMessage(content=\"Tell me a joke.\", source=\"user\")])\n",
    "print(\"Azure OpenAI Response:\", result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
