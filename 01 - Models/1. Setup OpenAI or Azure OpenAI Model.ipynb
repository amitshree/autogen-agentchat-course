{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Agents - Model Clients\n",
    "\n",
    "## Introduction\n",
    "In many AI applications, agents require access to LLM model services such as OpenAI, Azure OpenAI, or local models like Ollama. Since different providers have varying APIs, `autogen-core` implements a protocol for model clients, while `autogen-ext` provides implementations for popular services. `AgentChat` can use these model clients to interact with the model services seamlessly.\n",
    "\n",
    "This document provides an overview of how to integrate and use different model clients within an AI agent framework.\n",
    "\n",
    "---\n",
    "\n",
    "## OpenAI Model Client\n",
    "\n",
    "To access OpenAI models, install the `openai` extension:\n",
    "\n",
    "```bash\n",
    "pip install \"autogen-ext[openai]\"\n",
    "```\n",
    "\n",
    "### Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Response: Why did the scarecrow win an award?\n",
      "\n",
      "Because he was outstanding in his field!\n"
     ]
    }
   ],
   "source": [
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_core.models import UserMessage\n",
    "import asyncio\n",
    "\n",
    "openai_model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    # api_key=\"sk-...\",  # Optional if OPENAI_API_KEY environment variable is set.\n",
    ")\n",
    "\n",
    "result = await openai_model_client.create([UserMessage(content=\"Tell me a joke.\", source=\"user\")])\n",
    "print(\"OpenAI Response:\", result.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure OpenAI Model Client\n",
    "\n",
    "To use Azure OpenAI, install the required extensions:\n",
    "\n",
    "```bash\n",
    "pip install \"autogen-ext[openai,azure]\"\n",
    "```\n",
    "\n",
    "### Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from autogen_core.models import UserMessage\n",
    "import asyncio\n",
    "\n",
    "az_model_client = AzureOpenAIChatCompletionClient(\n",
    "    azure_deployment=\"{your-azure-deployment}\",\n",
    "    model=\"{model-name, such as gpt-4o}\",\n",
    "    api_version=\"2024-06-01\",\n",
    "    azure_endpoint=\"https://{your-custom-endpoint}.openai.azure.com/\",\n",
    "    api_key=\"sk-...\",  \n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "result = await az_model_client.create([UserMessage(content=\"Tell me a joke.\", source=\"user\")])\n",
    "print(\"Azure OpenAI Response:\", result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
